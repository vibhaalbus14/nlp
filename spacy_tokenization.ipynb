{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6towCASY3HX1"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "import re\n",
        "from spacy import Language\n",
        "from spacy.tokenizer import Tokenizer\n",
        "from spacy.symbols import ORTH\n",
        "from spacy.util import compile_infix_regex\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"hi there!I'm mr.Chunky.Born on 27th September 2023.Highly qualified cat...\"\\\n",
        "\"i'm naughty day in and day out.Very active and deadly.\"\\\n",
        "\"i sneak myself in and cajole everyone into handing over my food\"\\\n",
        "\"my sleep time is from 7:00 am to 11:00pm\"\\\n",
        "\"nah-uh,its typically the entire day ;)\""
      ],
      "metadata": {
        "id": "hoSUwgUd5mma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp=spacy.load(\"en_core_web_sm\")\n",
        "#loading the model and using its features\n",
        "\n",
        "doc=nlp(text)\n",
        "print(doc)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j0IpBqJc5Ihe",
        "outputId": "f991eccd-d5a9-4be1-df9b-f218334cbb48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hi there!I'm mr.Chunky.Born on 27th September 2023.Highly qualified cat...i'm naughty day in and day out.Very active and deadly.i sneak myself in and cajole everyone into handing over my foodmy sleep time is from 7:00 am to 11:00pmnah-uh,its typically the entire day ;)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens=[token.text for token in doc]\n",
        "print(tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gw5s_hXm6ami",
        "outputId": "a98e42c1-2fe1-447c-a2a2-8f86360bdeff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hi', \"there!I'm\", 'mr', '.', 'Chunky', '.', 'Born', 'on', '27th', 'September', '2023.Highly', 'qualified', 'cat', '...', \"i'm\", 'naughty', 'day', 'in', 'and', 'day', 'out', '.', 'Very', 'active', 'and', 'deadly.i', 'sneak', 'myself', 'in', 'and', 'cajole', 'everyone', 'into', 'handing', 'over', 'my', 'foodmy', 'sleep', 'time', 'is', 'from', '7:00', 'am', 'to', '11:00pmnah', '-', 'uh', ',', 'its', 'typically', 'the', 'entire', 'day', ';)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#add custom tokenisation\n",
        "#1.create a custom tokenisation function\n",
        "#2.create rules as regular expressions as infixes\n",
        "#3.compile infixes using compile_infix_regex\n",
        "#4.return an instance of token with nlp.vocab for storage of lexical items and infex_finditer to  identify the infixes\n",
        "\n",
        "def custom_token(nlp):\n",
        "  infix=(\n",
        "        r\"(?=<[0-9])[+\\-\\*^](?=[0-9-])\",\n",
        "            )\n",
        "  infix_final=compile_infix_regex(infix)\n",
        "  return Tokenizer(nlp.vocab,infix_finditer=infix_final.finditer)\n",
        "\n"
      ],
      "metadata": {
        "id": "Geu0z8iD6y54"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#loading the model\n",
        "#making the nlp tokenizer use our custom tokenizer\n",
        "\n",
        "nlp=spacy.load(\"en_core_web_sm\")\n",
        "nlp.tokenizer=custom_token(nlp)\n",
        "print(nlp)\n",
        "type(nlp)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "nPy2NkCu8Wl0",
        "outputId": "6e0a8b8f-3e51-4b8f-af0f-ee58e23e1b43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<spacy.lang.en.English object at 0x7bd4d72baa70>\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.lang.en.English"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>spacy.lang.en.English</b><br/>def __call__(text: Union[str, Doc], *, disable: Iterable[str]=SimpleFrozenList(), component_cfg: Optional[Dict[str, Dict[str, Any]]]=None) -&gt; Doc</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/usr/local/lib/python3.10/dist-packages/spacy/lang/en/__init__.py</a>A text-processing pipeline. Usually you&#x27;ll load this once per process,\n",
              "and pass the instance around your application.\n",
              "\n",
              "Defaults (class): Settings, data and factory methods for creating the `nlp`\n",
              "    object and processing pipeline.\n",
              "lang (str): IETF language code, such as &#x27;en&#x27;.\n",
              "\n",
              "DOCS: https://spacy.io/api/language</pre>\n",
              "      <script>\n",
              "      if (google.colab.kernel.accessAllowed && google.colab.files && google.colab.files.view) {\n",
              "        for (const element of document.querySelectorAll('.filepath')) {\n",
              "          element.style.display = 'block'\n",
              "          element.onclick = (event) => {\n",
              "            event.preventDefault();\n",
              "            event.stopPropagation();\n",
              "            google.colab.files.view(element.textContent, 22);\n",
              "          };\n",
              "        }\n",
              "      }\n",
              "      for (const element of document.querySelectorAll('.function-repr-contents')) {\n",
              "        element.onclick = (event) => {\n",
              "          event.preventDefault();\n",
              "          event.stopPropagation();\n",
              "          element.classList.toggle('function-repr-contents-collapsed');\n",
              "        };\n",
              "      }\n",
              "      </script>\n",
              "      </div>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pass the text to nlp and obtain doc\n",
        "doc=nlp(text)\n",
        "print(doc)\n",
        "\n",
        "words=[word.text for word in doc]\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SvipmOSi-_XV",
        "outputId": "798ddfa3-15ac-4b45-d988-9722e02884d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hi there!I'm mr.Chunky.Born on 27th September 2023.Highly qualified cat...i'm naughty day in and day out.Very active and deadly.i sneak myself in and cajole everyone into handing over my foodmy sleep time is from 7:00 am to 11:00pmnah-uh,its typically the entire day ;)\n",
            "['hi', \"there!I'm\", 'mr.Chunky.Born', 'on', '27th', 'September', '2023.Highly', 'qualified', \"cat...i'm\", 'naughty', 'day', 'in', 'and', 'day', 'out.Very', 'active', 'and', 'deadly.i', 'sneak', 'myself', 'in', 'and', 'cajole', 'everyone', 'into', 'handing', 'over', 'my', 'foodmy', 'sleep', 'time', 'is', 'from', '7:00', 'am', 'to', '11:00pmnah-uh,its', 'typically', 'the', 'entire', 'day', ';)']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#to understand coloquialisms or other texts,special cases can be added\n",
        "text2=\"i wanna go play today.\"\\\n",
        "\"lemme tell you,playing kepts u active and releases endorphins\"\\\n",
        "\"endorphins are hapy harmones\"\\\n",
        "\"it is C.P.U for physical activity\"\n",
        "\n"
      ],
      "metadata": {
        "id": "popiIAkx_dkU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#creating custom cases that are list of dictionaries using orth variable\n",
        "lemme_case=[{ORTH:\"lem\"},{ORTH:\"me\"}]\n",
        "wanna_case=[{ORTH:\"wan\"},{ORTH:\"na\"}]\n",
        "cpu_case=[{ORTH:\"C.P.U\"}]\n",
        "\n",
        "nlp.tokenizer.add_special_case(\"lemme\",lemme_case)\n",
        "nlp.tokenizer.add_special_case(\"wanna\",wanna_case)\n",
        "nlp.tokenizer.add_special_case(\"C.P.U\",cpu_case)\n"
      ],
      "metadata": {
        "id": "FCLItutO_3l0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "doc=nlp(text2)\n",
        "words=[(words.text,words.orth_) for words in doc]\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UPg33aZ5A_Ew",
        "outputId": "e9303339-ddef-4162-958b-36c590a00127"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('i', 'i'), ('wan', 'wan'), ('na', 'na'), ('go', 'go'), ('play', 'play'), ('today.lemme', 'today.lemme'), ('tell', 'tell'), ('you,playing', 'you,playing'), ('kepts', 'kepts'), ('u', 'u'), ('active', 'active'), ('and', 'and'), ('releases', 'releases'), ('endorphinsendorphins', 'endorphinsendorphins'), ('are', 'are'), ('hapy', 'hapy'), ('harmonesit', 'harmonesit'), ('is', 'is'), ('C.P.U', 'C.P.U'), ('for', 'for'), ('physical', 'physical'), ('activity', 'activity')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text3=\"hello...im vibha...learning spacy... and nltk\""
      ],
      "metadata": {
        "id": "HhLJ2evzCPDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@Language.component(\"custom_elipses_sent\")\n",
        "\n",
        "def custom_elipses_sent(doc):\n",
        "  for token in doc[:-1]:\n",
        "    if token.text==\"...\":\n",
        "      doc[token.i  + 1].is_sent_start=True\n",
        "  return doc\n",
        "\n",
        "nlp.add_pipe(\"custom_elipses_sent\",before=\"parser\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "ATFcHJ5bCYoA",
        "outputId": "e2b436a7-2c39-4101-f77e-7cfcfe1de32c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.custom_elipses_sent(doc)>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>custom_elipses_sent</b><br/>def custom_elipses_sent(doc)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/content/&lt;ipython-input-37-3a1a407ae9e9&gt;</a>&lt;no docstring&gt;</pre></div>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp=spacy.load(\"en_core_web_sm\")\n",
        "nlp.tokenizer=custom_token(nlp)\n",
        "doc=nlp(text3)\n",
        "for index,sentence in enumerate(doc.sents,start=1):\n",
        "  print(f'Sentence {index}:{sentence}')\n",
        "for token in doc:\n",
        "  print(token.text)\n",
        "  print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2WxeXCLiDT4u",
        "outputId": "b981030b-2252-431d-ae32-e1a098352a3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence 1:hello...im vibha...learning spacy... and nltk\n",
            "hello...im\n",
            "\n",
            "vibha...learning\n",
            "\n",
            "spacy...\n",
            "\n",
            "and\n",
            "\n",
            "nltk\n",
            "\n"
          ]
        }
      ]
    }
  ]
}